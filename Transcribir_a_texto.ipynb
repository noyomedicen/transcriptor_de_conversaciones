{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO64WLZyas3uaSMkeq3Jdid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noyomedicen/transcriptor_de_conversaciones/blob/main/Transcribir_a_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar librerias"
      ],
      "metadata": {
        "id": "qFAIB5_aEnvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install torch\n",
        "!pip install pydub\n",
        "!apt-get install ffmpeg\n",
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "FEv7gYnoY1jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar librerias"
      ],
      "metadata": {
        "id": "YYfv22aVEu3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "import math\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "VL-_7uaERsyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s48Fp-6WR1Ae",
        "outputId": "d8cd79f4-15c0-400a-c024-383b1fcaf6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unificar audios\n"
      ],
      "metadata": {
        "id": "qEk1z48RxnDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta de la carpeta con los archivos de audio\n",
        "audios_path = 'Tu_ruta_aca'"
      ],
      "metadata": {
        "id": "ZsjR4NE2mmQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir archivos M4A a MP4\n",
        "m4a_files = [f for f in os.listdir(audios_path) if f.endswith('.m4a')]\n",
        "\n",
        "for m4a_file in m4a_files:\n",
        "    input_file = os.path.join(audios_path, m4a_file)\n",
        "    mp4_file = os.path.splitext(m4a_file)[0] + '.mp4'\n",
        "    output_file = os.path.join(audios_path, mp4_file)\n",
        "\n",
        "    # Comando FFmpeg para convertir M4A a MP4\n",
        "    subprocess.call(['ffmpeg', '-i', input_file, '-c', 'copy', output_file])"
      ],
      "metadata": {
        "id": "IMwngMjAl4bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de archivos mp4 en la carpeta\n",
        "audio_files = sorted([f for f in os.listdir(audios_path) if f.endswith('.mp4')], key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "# Crear un objeto de audio unificado\n",
        "unified_audio = AudioSegment.empty()"
      ],
      "metadata": {
        "id": "xPRDUEaJwsYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir los archivos en uno solo\n",
        "for audio_file in audio_files:\n",
        "    audio_path = os.path.join(audios_path, audio_file)\n",
        "    audio = AudioSegment.from_file(audio_path, format='mp4')\n",
        "    unified_audio += audio\n",
        "\n",
        "# Guardar el archivo unificado\n",
        "unified_audio_path = os.path.join(audios_path, \"audio_unificado.mp4\")\n",
        "unified_audio.export(unified_audio_path, format='mp4')\n",
        "\n",
        "print(f\"Archivo unificado guardado en: {unified_audio_path}\")"
      ],
      "metadata": {
        "id": "LYSISEHhY4Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estandarizacion de audios"
      ],
      "metadata": {
        "id": "CBxwbRWfxtcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta de la carpeta donde se guardarán los audios divididos\n",
        "output_path = 'Tu_ruta'\n",
        "\n",
        "# Duración en milisegundos (5 minutos = 300000 ms)\n",
        "chunk_length_ms = 5 * 60 * 1000\n",
        "\n",
        "# Cargar el archivo de audio unificado\n",
        "unified_audio = AudioSegment.from_file(unified_audio_path, format='mp4')\n",
        "\n",
        "# Calcular cuántos fragmentos de 5 minutos se necesitan\n",
        "num_chunks = math.ceil(len(unified_audio) / chunk_length_ms)"
      ],
      "metadata": {
        "id": "teuLT_j_x0zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir y guardar los fragmentos\n",
        "for i in range(num_chunks):\n",
        "    start_time = i * chunk_length_ms\n",
        "    end_time = min((i + 1) * chunk_length_ms, len(unified_audio))\n",
        "    audio_chunk = unified_audio[start_time:end_time]\n",
        "\n",
        "    chunk_filename = f\"{i+1}.mp4\"\n",
        "    chunk_path = os.path.join(output_path, chunk_filename)\n",
        "\n",
        "    # Guardar el fragmento\n",
        "    audio_chunk.export(chunk_path, format='mp4')\n",
        "    print(f\"Guardado: {chunk_filename}\")"
      ],
      "metadata": {
        "id": "mFuZIlMuY-L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar todos los archivos en la carpeta 'Entrada'\n",
        "for file in os.listdir(audios_path):\n",
        "    file_path = os.path.join(audios_path, file)\n",
        "    try:\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Eliminado: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al eliminar {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "oMDRxT5OZBCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribir audios"
      ],
      "metadata": {
        "id": "Zc5Poc2h9o0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la API de OpenAI\n",
        "OPENAI_API_KEY = 'Tu_clave_de_openia'\n",
        "# Configurar el cliente asíncrono\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "gJSEN_vXMLyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta de los audios estandarizados (donde están los archivos de audio de 5 minutos)\n",
        "audios_path = 'Tu_ruta_aca'\n",
        "\n",
        "# Ruta donde se guardarán los archivos de texto\n",
        "text_output_path = 'Tu_ruta_aca'\n",
        "\n",
        "# Cargar el modelo Whisper\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wBut_PL9wQM",
        "outputId": "20fa7de6-0ed7-47cf-f88b-e05cab8b9d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 87.6MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para transcribir audios y mejorar la redacción\n",
        "def transcribe_and_improve_audios(audios_path):\n",
        "    n = 0\n",
        "    unified_transcription = \"\"\n",
        "\n",
        "    # Obtener lista de archivos de audio en la carpeta\n",
        "    audio_files = sorted(\n",
        "        [f for f in os.listdir(audios_path) if f.endswith('.mp4')],\n",
        "        key=lambda x: int(os.path.splitext(x)[0])\n",
        "    )\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        audio_path = os.path.join(audios_path, audio_file)\n",
        "\n",
        "        # Verificar si el archivo de audio existe\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Archivo de audio no encontrado: {audio_path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Transcribir el audio usando tu modelo (por ejemplo, Whisper)\n",
        "            result = model.transcribe(audio_path, fp16=False)\n",
        "            text = result['text']\n",
        "\n",
        "            print(f\"Audio transcrito: {audio_path}\")\n",
        "            print(f\"Texto: {text}\")\n",
        "\n",
        "            # Mejorar la redacción usando la API de OpenAI\n",
        "            retry_count = 0\n",
        "            max_retries = 2  # Número máximo de intentos\n",
        "\n",
        "            while retry_count < max_retries:\n",
        "                try:\n",
        "                    improved_text = improve_transcription(text)\n",
        "                    # Si la mejora es exitosa, salimos del bucle de reintento\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    retry_count += 1\n",
        "                    print(f\"Error al mejorar la transcripción: {e}\")\n",
        "                    if retry_count < max_retries:\n",
        "                        print(f\"Reintentando mejorar la transcripción ({retry_count}/{max_retries})...\")\n",
        "                    else:\n",
        "                        print(\"No se pudo mejorar el texto después de varios intentos.\")\n",
        "                        improved_text = \"No se pudo mejorar el texto debido a un error.\"\n",
        "\n",
        "            # Añadir el texto mejorado al archivo unificado\n",
        "            unified_transcription += f\"\\n\\n=== Transcripción de {audio_file} ===\\n\\n{improved_text}\\n\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al transcribir el audio {audio_file}: {e}\")\n",
        "\n",
        "    # Guardar el archivo unificado con todas las transcripciones mejoradas\n",
        "    today_date = datetime.today().strftime('%Y-%m-%d')\n",
        "    unified_txt_path = os.path.join(text_output_path, f'transcripcion_unificada_mejorada_{today_date}.txt')\n",
        "\n",
        "    with open(unified_txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(unified_transcription)\n",
        "    print(n)\n",
        "    print(f\"Transcripción unificada mejorada guardada en: {unified_txt_path}\")\n",
        "\n",
        "    # Generar el resumen final\n",
        "    summary = generate_summary(unified_transcription)\n",
        "\n",
        "    # Guardar el resumen en un archivo\n",
        "    summary_txt_path = os.path.join(text_output_path, f'resumen_{today_date}.txt')\n",
        "\n",
        "    with open(summary_txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "    print(f\"Resumen guardado en: {summary_txt_path}\")\n",
        "\n",
        "# Función para mejorar la transcripción utilizando la API de OpenAI\n",
        "def improve_transcription(text):\n",
        "    prompt = f\"\"\"He transcrito un fragmento de una reunión donde un líder de área explica un macroproceso. El texto puede contener errores de transcripción, interrupciones y preguntas de otros participantes.\n",
        "\n",
        "Tu tarea es:\n",
        "\n",
        "- Corregir los errores de transcripción y mejorar la redacción donde sea necesario.\n",
        "- Ignorar las interrupciones y preguntas que no aporten al contenido principal.\n",
        "- Unificar la información proporcionada por el líder del área sobre el macroproceso.\n",
        "\n",
        "Fragmento de la transcripción:\n",
        "\n",
        "{text}\n",
        "\n",
        "Por favor, proporciona el texto corregido y mejorado a continuación.\"\"\"\n",
        "\n",
        "    # Llamada a la API de OpenAI según tu formato específico\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # Asegúrate de que este es el modelo correcto\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{prompt}\",\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Procesar la respuesta de la API\n",
        "    improved_text = completion.choices[0].message.content.strip()\n",
        "    return improved_text\n",
        "\n",
        "# Función para generar el resumen\n",
        "def generate_summary(unified_transcription):\n",
        "    prompt = f\"\"\"He recopilado la siguiente transcripción mejorada de una reunión donde un líder de área explica un macroproceso:\n",
        "\n",
        "{unified_transcription}\n",
        "\n",
        "Por favor, genera un resumen claro y conciso de los puntos principales discutidos por el líder del área.\"\"\"\n",
        "\n",
        "    # Llamada a la API de OpenAI según tu formato específico\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{prompt}\",\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Procesar la respuesta de la API\n",
        "    summary = completion.choices[0].message.content.strip()\n",
        "    return summary"
      ],
      "metadata": {
        "id": "3aG2Zhdx9odk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para transcribir y mejorar los audios\n",
        "transcribe_and_improve_audios(audios_path)"
      ],
      "metadata": {
        "id": "j_4RArhfZE_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar todos los archivos en la carpeta 'Entrada'\n",
        "for file in os.listdir(audios_path):\n",
        "    file_path = os.path.join(audios_path, file)\n",
        "    try:\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Eliminado: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al eliminar {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "Snz7YM30ZHOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}